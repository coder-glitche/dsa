# -*- coding: utf-8 -*-
"""Untitled112.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18dscI226jedhDzlP9oVNSFCLfjtvaeZJ
"""

import re

# Create a dummy data.txt file for demonstration purposes
# In a real scenario, you would upload your data.txt or create its content here.
with open("data.txt", "w") as f:
    f.write("Here is some sample text with emails:\n")
    f.write("Contact support@example.com for help. Visit our site at www.test.org.\n")
    f.write("My email is user123@domain.co.uk and her email is another.user@mail.net.\n")

with open("data.txt", "r") as f:
    text = f.read()

# Example: extract all email IDs
emails = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", text)
phone_numbers = re.findall(r'\+?\d{1,3}[- .\(]?\d{3}[- .\)]?\d{3}[- .]?\d{4}', text)

print("Extracted Phone Numbers:")
print(phone_numbers)
print("Extracted Emails:")
print(emails)

n = int(input("Enter n: "))
lst = []

for i in range(n):
    lst.append(int(input("Enter value: ")))

if lst.count(19) == 2 and lst.count(5) >= 3:
    print("Valid List:", lst)
else:
    print("Invalid List:", lst)

import re

text = "apple, mango, banana, grapes"

words = re.findall(r"[A-Za-z]+", text)
separators = re.findall(r"[, ]+", text)

print("Words:", words)
print("Separators:", separators)

import re

# Create a dummy numbers.txt file for demonstration
# In a real scenario, you would upload your numbers.txt or create its content here.
with open("numbers.txt", "w") as f:
    f.write("10 20 30\n")
    f.write("40\n")
    f.write("50\n")

with open("numbers.txt","r") as f:
    data = f.read()

nums = re.findall(r"\d+", data)
nums = list(map(int, nums))

print("Numbers:", nums)
print("Sum:", sum(nums))

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

nltk.download("punkt")
nltk.download("stopwords")
nltk.download("punkt_tab") # Added this line to download the missing resource

text = "This product is really amazing and works very well."

# Tokenization
tokens = nltk.word_tokenize(text)

# Stopword removal
stop_words = set(stopwords.words("english"))
filtered = [w for w in tokens if w.lower() not in stop_words]

# Stemming
ps = PorterStemmer()
stemmed = [ps.stem(w) for w in filtered]

print("Tokens:", tokens)
print("After Stopword Removal:", filtered)
print("After Stemming:", stemmed)

import matplotlib.pyplot as plt
from collections import Counter
import re

text = "python python is great and python is easy to learn because python is powerful"

words = re.findall(r"\w+", text.lower())
freq = Counter(words)
print(freq)

x=[e for e in freq.keys()]
y=[e for e in freq.values()]
plt.bar(x,y)
plt.title("Most Frequent Word")
plt.xlabel("Word")
plt.ylabel("Frequency")
plt.show()

import nltk
from nltk.util import ngrams
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')

text = "machine learning is fun and machine learning is powerful"
stop_words = set(stopwords.words("english"))

tokens = [w for w in nltk.word_tokenize(text.lower()) if w not in stop_words]

bigrams = list(ngrams(tokens, 2))

word = "learning"
candidates = [b[1] for b in bigrams if b[0] == word]

print("Next word predictions after 'learning':", candidates)

from collections import Counter

text = "the cat sat on the mat"
words = text.split()

count = Counter(words)
vocab = len(count)

word = "cat"
prob = (count[word] + 1) / (len(words) + vocab)

print("Laplace Smoothed Probability of 'cat' =", prob)

import nltk

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger_eng') # Corrected resource name

sentence = "The quick brown fox jumps over the lazy dog"
tokens = nltk.word_tokenize(sentence)
pos_tags = nltk.pos_tag(tokens)

print("POS Tags:", pos_tags)

# Simple grammar for parsing
grammar = "NP: {<DT>?<JJ>*<NN>}"
parser = nltk.RegexpParser(grammar)
tree = parser.parse(pos_tags)
# tree.draw() # Removed this line as it causes TclError in headless environments
print("Parse Tree:\n", tree)

import re
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')

text = "Follow us @OpenAI and @ChatGPT! AI is amazing, and AI helps everyone."

handles = re.findall(r"@\w+", text)
print("Twitter Handles:", handles)

words = re.findall(r"\w+", text.lower())
stop = set(stopwords.words("english"))
filtered = [w for w in words if w not in stop]

freq = Counter(filtered).most_common(5)

# Plot
labels, counts = zip(*freq)
plt.bar(labels, counts)
plt.title("Top Word Frequencies")
plt.show()

from nltk.util import ngrams
from collections import Counter
import nltk

docs = [
    "machine learning is fun",
    "learning machines make predictions",
    "predictions are useful in machine learning"
]

tokens = []
for d in docs:
    tokens.extend(nltk.word_tokenize(d.lower()))

bigrams = list(ngrams(tokens, 2))
freq = Counter(bigrams)

print("Total unique bi-grams:", len(freq))
print("Top 5 bi-grams:", freq.most_common(5))

